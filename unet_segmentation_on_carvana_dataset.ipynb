{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanreed1111/cnn-demos/blob/main/unet_segmentation_on_carvana_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q torchmetrics albumentations pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FGE6kaA5uBM",
        "outputId": "38f00a84-da9c-4aa5-dc51-b5190cb25529"
      },
      "id": "6FGE6kaA5uBM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 419 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 707 kB 33.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 41.5 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.10.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: https://www.kaggle.com/code/alanyu223/unet-segmentation-on-carvana-dataset"
      ],
      "metadata": {
        "id": "d71kqVK35h4v"
      },
      "id": "d71kqVK35h4v"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "68beaf88",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-15T00:11:33.247420Z",
          "iopub.status.busy": "2022-06-15T00:11:33.246916Z",
          "iopub.status.idle": "2022-06-15T00:11:42.468394Z",
          "shell.execute_reply": "2022-06-15T00:11:42.467535Z"
        },
        "papermill": {
          "duration": 9.230058,
          "end_time": "2022-06-15T00:11:42.470717",
          "exception": false,
          "start_time": "2022-06-15T00:11:33.240659",
          "status": "completed"
        },
        "tags": [],
        "id": "68beaf88"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import math\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.utils\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics as tm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from typing import List\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7686ad2c",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-06-15T00:11:42.480760Z",
          "iopub.status.busy": "2022-06-15T00:11:42.480097Z",
          "iopub.status.idle": "2022-06-15T00:11:42.536402Z",
          "shell.execute_reply": "2022-06-15T00:11:42.535695Z"
        },
        "papermill": {
          "duration": 0.063089,
          "end_time": "2022-06-15T00:11:42.538301",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.475212",
          "status": "completed"
        },
        "tags": [],
        "id": "7686ad2c"
      },
      "outputs": [],
      "source": [
        "# # Check for GPU or setup TPU\n",
        "# if not torch.cuda.is_available():\n",
        "#     !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#     !python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n",
        "#     !pip install pytorch-lightning==1.1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb17eae",
      "metadata": {
        "papermill": {
          "duration": 0.003682,
          "end_time": "2022-06-15T00:11:42.545876",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.542194",
          "status": "completed"
        },
        "tags": [],
        "id": "feb17eae"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "The following image depicts the architecture that will be used for segmentation. We will first define a torch module as the building block for the model, then use a pytorch lightning module to define the final model.\n",
        "\n",
        "![](https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Pytorch/image_segmentation/semantic_segmentation_unet/UNET_architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "746c88f8",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-06-15T00:11:42.554743Z",
          "iopub.status.busy": "2022-06-15T00:11:42.554437Z",
          "iopub.status.idle": "2022-06-15T00:11:42.579678Z",
          "shell.execute_reply": "2022-06-15T00:11:42.578855Z"
        },
        "papermill": {
          "duration": 0.032139,
          "end_time": "2022-06-15T00:11:42.581727",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.549588",
          "status": "completed"
        },
        "tags": [],
        "id": "746c88f8"
      },
      "outputs": [],
      "source": [
        "class Block(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias= False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias= False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(pl.LightningModule):\n",
        "    def __init__(self, in_channels: int=3, out_channels: int=1, features: List=[64,128,256,512], learning_rate=1.5e-3):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate \n",
        "        self.down = nn.ModuleList()\n",
        "        self.up = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        for feature in features:\n",
        "            self.down.append(Block(in_channels, feature))\n",
        "            in_channels=feature\n",
        "        for feature in reversed(features):\n",
        "            self.up.append(\n",
        "                nn.ConvTranspose2d(feature*2, feature, 2, 2)\n",
        "            )\n",
        "            self.up.append(\n",
        "                Block(feature*2, feature) # x gets concat to 2xchannel\n",
        "            )\n",
        "        self.bottleneck = Block(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, 1)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "        self.val_num_correct = 0\n",
        "        self.val_num_pixels = 0\n",
        "        self.val_dice_score = 0\n",
        "        self.num_correct = 0\n",
        "        self.num_pixels = 0\n",
        "        self.dice_score = 0\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        for down in self.down:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "        for idx in range(0, len(self.up), 2):\n",
        "            x = self.up[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1) # Concat along channels (b, c, h, w)\n",
        "            x = self.up[idx+1](concat_skip)\n",
        "        return self.final_conv(x)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        pred = self(x)\n",
        "        loss = self.loss_fn(pred, y)\n",
        "        pred = torch.sigmoid(pred)\n",
        "        pred = (pred > 0.5).float()\n",
        "        self.num_correct += (pred == y).sum()\n",
        "        self.num_pixels += torch.numel(pred)\n",
        "        self.dice_score += (2 * (pred * y).sum()) / (\n",
        "            (pred + y).sum() + 1e-8\n",
        "        )\n",
        "        self.log('train_loss', loss, logger = True)\n",
        "        return {'loss': loss}\n",
        "    \n",
        "    def training_epoch_end(self, output):\n",
        "        train_acc = float(f'{(self.num_correct/self.num_pixels)*100:.2f}')\n",
        "        self.log('train_acc', train_acc, prog_bar = True, logger = True)\n",
        "        dice_score = self.dice_score/len(output)\n",
        "        self.log('train_dice_score', dice_score, prog_bar = True, logger = True)\n",
        "        self.num_correct, self.num_pixels, self.dice_score = 0,0,0\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        pred = self(x)\n",
        "        loss = self.loss_fn(pred, y)\n",
        "        pred = torch.sigmoid(pred)\n",
        "        pred = (pred > 0.5).float()\n",
        "        self.val_num_correct += (pred == y).sum()\n",
        "        self.val_num_pixels += torch.numel(pred)\n",
        "        self.val_dice_score += (2 * (pred * y).sum()) / (\n",
        "            (pred + y).sum() + 1e-8\n",
        "        )\n",
        "        self.log('val_loss', loss, prog_bar = True, logger = True)\n",
        "        return {'loss': loss}\n",
        "    \n",
        "    def validation_epoch_end(self, output):\n",
        "        val_acc = float(f'{(self.val_num_correct/self.val_num_pixels)*100:.2f}')\n",
        "        self.log('val_acc', val_acc, prog_bar = True, logger = True)\n",
        "        dice_score = self.val_dice_score/len(output)\n",
        "        self.log('val_dice_score', dice_score, prog_bar = True, logger = True)\n",
        "        self.val_num_correct, self.val_num_pixels, self.val_dice_score = 0,0,0\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(params = self.parameters(), lr = self.learning_rate, weight_decay=0.3)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8818e5cc",
      "metadata": {
        "papermill": {
          "duration": 0.00361,
          "end_time": "2022-06-15T00:11:42.589093",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.585483",
          "status": "completed"
        },
        "tags": [],
        "id": "8818e5cc"
      },
      "source": [
        "We can see how many parameters we have in our model using the following command. Note that the order shown does not reflect how a training example flows through the architecture. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e3155561",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-15T00:11:42.598054Z",
          "iopub.status.busy": "2022-06-15T00:11:42.597771Z",
          "iopub.status.idle": "2022-06-15T00:11:42.927440Z",
          "shell.execute_reply": "2022-06-15T00:11:42.926610Z"
        },
        "papermill": {
          "duration": 0.337443,
          "end_time": "2022-06-15T00:11:42.930318",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.592875",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3155561",
        "outputId": "eef5b8f9-5789-4854-9788-dfed2b6abf02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   | Name              | Type              | Params\n",
              "---------------------------------------------------------\n",
              "0  | down              | ModuleList        | 4.7 M \n",
              "1  | down.0            | Block             | 38.8 K\n",
              "2  | down.0.conv       | Sequential        | 38.8 K\n",
              "3  | down.0.conv.0     | Conv2d            | 1.7 K \n",
              "4  | down.0.conv.1     | BatchNorm2d       | 128   \n",
              "5  | down.0.conv.2     | ReLU              | 0     \n",
              "6  | down.0.conv.3     | Conv2d            | 36.9 K\n",
              "7  | down.0.conv.4     | BatchNorm2d       | 128   \n",
              "8  | down.0.conv.5     | ReLU              | 0     \n",
              "9  | down.1            | Block             | 221 K \n",
              "10 | down.1.conv       | Sequential        | 221 K \n",
              "11 | down.1.conv.0     | Conv2d            | 73.7 K\n",
              "12 | down.1.conv.1     | BatchNorm2d       | 256   \n",
              "13 | down.1.conv.2     | ReLU              | 0     \n",
              "14 | down.1.conv.3     | Conv2d            | 147 K \n",
              "15 | down.1.conv.4     | BatchNorm2d       | 256   \n",
              "16 | down.1.conv.5     | ReLU              | 0     \n",
              "17 | down.2            | Block             | 885 K \n",
              "18 | down.2.conv       | Sequential        | 885 K \n",
              "19 | down.2.conv.0     | Conv2d            | 294 K \n",
              "20 | down.2.conv.1     | BatchNorm2d       | 512   \n",
              "21 | down.2.conv.2     | ReLU              | 0     \n",
              "22 | down.2.conv.3     | Conv2d            | 589 K \n",
              "23 | down.2.conv.4     | BatchNorm2d       | 512   \n",
              "24 | down.2.conv.5     | ReLU              | 0     \n",
              "25 | down.3            | Block             | 3.5 M \n",
              "26 | down.3.conv       | Sequential        | 3.5 M \n",
              "27 | down.3.conv.0     | Conv2d            | 1.2 M \n",
              "28 | down.3.conv.1     | BatchNorm2d       | 1.0 K \n",
              "29 | down.3.conv.2     | ReLU              | 0     \n",
              "30 | down.3.conv.3     | Conv2d            | 2.4 M \n",
              "31 | down.3.conv.4     | BatchNorm2d       | 1.0 K \n",
              "32 | down.3.conv.5     | ReLU              | 0     \n",
              "33 | up                | ModuleList        | 12.2 M\n",
              "34 | up.0              | ConvTranspose2d   | 2.1 M \n",
              "35 | up.1              | Block             | 7.1 M \n",
              "36 | up.1.conv         | Sequential        | 7.1 M \n",
              "37 | up.1.conv.0       | Conv2d            | 4.7 M \n",
              "38 | up.1.conv.1       | BatchNorm2d       | 1.0 K \n",
              "39 | up.1.conv.2       | ReLU              | 0     \n",
              "40 | up.1.conv.3       | Conv2d            | 2.4 M \n",
              "41 | up.1.conv.4       | BatchNorm2d       | 1.0 K \n",
              "42 | up.1.conv.5       | ReLU              | 0     \n",
              "43 | up.2              | ConvTranspose2d   | 524 K \n",
              "44 | up.3              | Block             | 1.8 M \n",
              "45 | up.3.conv         | Sequential        | 1.8 M \n",
              "46 | up.3.conv.0       | Conv2d            | 1.2 M \n",
              "47 | up.3.conv.1       | BatchNorm2d       | 512   \n",
              "48 | up.3.conv.2       | ReLU              | 0     \n",
              "49 | up.3.conv.3       | Conv2d            | 589 K \n",
              "50 | up.3.conv.4       | BatchNorm2d       | 512   \n",
              "51 | up.3.conv.5       | ReLU              | 0     \n",
              "52 | up.4              | ConvTranspose2d   | 131 K \n",
              "53 | up.5              | Block             | 442 K \n",
              "54 | up.5.conv         | Sequential        | 442 K \n",
              "55 | up.5.conv.0       | Conv2d            | 294 K \n",
              "56 | up.5.conv.1       | BatchNorm2d       | 256   \n",
              "57 | up.5.conv.2       | ReLU              | 0     \n",
              "58 | up.5.conv.3       | Conv2d            | 147 K \n",
              "59 | up.5.conv.4       | BatchNorm2d       | 256   \n",
              "60 | up.5.conv.5       | ReLU              | 0     \n",
              "61 | up.6              | ConvTranspose2d   | 32.8 K\n",
              "62 | up.7              | Block             | 110 K \n",
              "63 | up.7.conv         | Sequential        | 110 K \n",
              "64 | up.7.conv.0       | Conv2d            | 73.7 K\n",
              "65 | up.7.conv.1       | BatchNorm2d       | 128   \n",
              "66 | up.7.conv.2       | ReLU              | 0     \n",
              "67 | up.7.conv.3       | Conv2d            | 36.9 K\n",
              "68 | up.7.conv.4       | BatchNorm2d       | 128   \n",
              "69 | up.7.conv.5       | ReLU              | 0     \n",
              "70 | pool              | MaxPool2d         | 0     \n",
              "71 | bottleneck        | Block             | 14.2 M\n",
              "72 | bottleneck.conv   | Sequential        | 14.2 M\n",
              "73 | bottleneck.conv.0 | Conv2d            | 4.7 M \n",
              "74 | bottleneck.conv.1 | BatchNorm2d       | 2.0 K \n",
              "75 | bottleneck.conv.2 | ReLU              | 0     \n",
              "76 | bottleneck.conv.3 | Conv2d            | 9.4 M \n",
              "77 | bottleneck.conv.4 | BatchNorm2d       | 2.0 K \n",
              "78 | bottleneck.conv.5 | ReLU              | 0     \n",
              "79 | final_conv        | Conv2d            | 65    \n",
              "80 | loss_fn           | BCEWithLogitsLoss | 0     \n",
              "---------------------------------------------------------\n",
              "31.0 M    Trainable params\n",
              "0         Non-trainable params\n",
              "31.0 M    Total params\n",
              "124.151   Total estimated model params size (MB)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "pl.utilities.model_summary.summarize(UNET(),-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8431d99a",
      "metadata": {
        "papermill": {
          "duration": 0.004147,
          "end_time": "2022-06-15T00:11:42.939467",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.935320",
          "status": "completed"
        },
        "tags": [],
        "id": "8431d99a"
      },
      "source": [
        "# Data\n",
        "Now that we have our model, we define a torch Dataset to retrieve and apply our transformations to our data. We define the three necessary methods and move on to defining our LightningDataModule, which will split our Dataset into train/val splits and prepare the appropriate Dataloader when called by the Trainer object later on when we start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "94e218f0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-15T00:11:42.949437Z",
          "iopub.status.busy": "2022-06-15T00:11:42.948645Z",
          "iopub.status.idle": "2022-06-15T00:11:42.957690Z",
          "shell.execute_reply": "2022-06-15T00:11:42.956858Z"
        },
        "papermill": {
          "duration": 0.01577,
          "end_time": "2022-06-15T00:11:42.959411",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.943641",
          "status": "completed"
        },
        "tags": [],
        "id": "94e218f0"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, image_path, mask_path, transforms):\n",
        "    self.images = glob.glob(os.path.join(image_path, '*.jpg'))\n",
        "    self.image_path = image_path\n",
        "    self.mask_path = mask_path\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img = np.array(Image.open(self.images[idx]).convert('RGB'))\n",
        "    mask = np.array(Image.open(os.path.join(self.mask_path, os.path.basename(self.images[idx]).replace('.jpg', '.png')))) \n",
        "    mask[mask == 255.0] = 1.0  \n",
        "    augmentations = self.transforms(image=img, mask=mask)\n",
        "    image = augmentations[\"image\"]\n",
        "    mask = augmentations[\"mask\"]\n",
        "    mask = torch.unsqueeze(mask, 0)\n",
        "    mask = mask.type(torch.float32)\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9939b20b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-15T00:11:42.968980Z",
          "iopub.status.busy": "2022-06-15T00:11:42.968661Z",
          "iopub.status.idle": "2022-06-15T00:11:42.978888Z",
          "shell.execute_reply": "2022-06-15T00:11:42.978239Z"
        },
        "papermill": {
          "duration": 0.01693,
          "end_time": "2022-06-15T00:11:42.980476",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.963546",
          "status": "completed"
        },
        "tags": [],
        "id": "9939b20b"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataModule(pl.LightningDataModule):\n",
        "    \n",
        "    def __init__(self, image_path, mask_path, transform, train_size=0.90, batch_size: int = 9):\n",
        "        super().__init__()\n",
        "        self.image_path = image_path\n",
        "        self.mask_path = mask_path\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transform\n",
        "        self.train_size = train_size\n",
        "        \n",
        "    def setup(self, stage = None):\n",
        "        if stage in (None, 'fit'):\n",
        "            ds = SegmentationDataset(self.image_path, self.mask_path, self.transform)\n",
        "            train_size = math.floor(len(ds)*self.train_size)\n",
        "            val_size = len(ds)-train_size\n",
        "            train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
        "            self.train_dataset = train_ds\n",
        "            self.val_dataset = val_ds\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, self.batch_size, num_workers=2, shuffle = True, persistent_workers=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, self.batch_size, num_workers=2, persistent_workers=True)\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, self.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b4a121",
      "metadata": {
        "papermill": {
          "duration": 0.004034,
          "end_time": "2022-06-15T00:11:42.988582",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.984548",
          "status": "completed"
        },
        "tags": [],
        "id": "b3b4a121"
      },
      "source": [
        "# Training\n",
        "\n",
        "Once our model, optimizer+loss, and LightningDataModule is defined, we can use Trainer to run our training and validation loops. We can also use callbacks provided by lightning (check out [Bolts](https://lightning-bolts.readthedocs.io/en/latest/) for advanced callbacks, such as for sparsification) to create checkpoints or for early stopping. Trainer also takes care of multi-GPU and TPU training. \n",
        "\n",
        "We also use albumentations to apply the **same** data augmentations on the image **and** mask. I don't believe the torchvision transforms allows you to do the same, but correct me if I'm wrong. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Must Download data from Kaggle to run the rest of the notebook"
      ],
      "metadata": {
        "id": "ZnN-0l3l6lQK"
      },
      "id": "ZnN-0l3l6lQK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1544455f",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-06-15T00:11:42.998135Z",
          "iopub.status.busy": "2022-06-15T00:11:42.997434Z",
          "iopub.status.idle": "2022-06-15T01:02:29.758797Z",
          "shell.execute_reply": "2022-06-15T01:02:29.757978Z"
        },
        "papermill": {
          "duration": 3046.768417,
          "end_time": "2022-06-15T01:02:29.760954",
          "exception": false,
          "start_time": "2022-06-15T00:11:42.992537",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "90f845cbaa684884bfd0e56f10c2f5dc",
            "e7183afd1ccf4e02ad41e6a0d0bf2d8d",
            "d2ddfc9ecc5c43ae9e62821c23ff0af5",
            "d9df64c6e3794757ab4c987cc052dc05",
            "c4edcf80c89049ad845e22b72f5e5291",
            "9bc5abe1ecfe45ec8db03b727be95dba",
            "e13565637a6a4a2189cf425177273e79",
            "496a17b94ad1413eb267ffee9ddc3141",
            "d59cdb0d3af14a348deb37628afe29ac",
            "8bf3a523d4224f3d93b05a41a386ce9c"
          ]
        },
        "id": "1544455f",
        "outputId": "cd09d8c2-111c-407c-c9c1-6355026f9e6a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90f845cbaa684884bfd0e56f10c2f5dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7183afd1ccf4e02ad41e6a0d0bf2d8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2ddfc9ecc5c43ae9e62821c23ff0af5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9df64c6e3794757ab4c987cc052dc05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4edcf80c89049ad845e22b72f5e5291",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bc5abe1ecfe45ec8db03b727be95dba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e13565637a6a4a2189cf425177273e79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "496a17b94ad1413eb267ffee9ddc3141",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d59cdb0d3af14a348deb37628afe29ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bf3a523d4224f3d93b05a41a386ce9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=360, width=480),\n",
        "        A.Rotate(limit=45, p=0.7),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.3),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        A.pytorch.ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "ds = SegmentationDataModule('../input/carvana-image-masking-png/train_images', '../input/carvana-image-masking-png/train_masks', transform=transform)\n",
        "model = UNET()\n",
        "if not os.path.isdir('./unet_3'): os.mkdir('./unet_3')\n",
        "checkpointCallback = pl.callbacks.ModelCheckpoint(dirpath=\"./unet_3\", \n",
        "                                                  save_top_k=1, \n",
        "                                                  monitor=\"val_loss\",\n",
        "                                                 filename='{epoch}-{val_loss:.5f}',\n",
        "                                                 mode='min')\n",
        "if torch.cuda.is_available():\n",
        "    trainer = pl.Trainer(max_epochs=8, accelerator='gpu', gpus=1, \n",
        "                         callbacks=[checkpointCallback], profiler='simple',\n",
        "                         auto_lr_find=True)\n",
        "else:\n",
        "    trainer = pl.Trainer(max_epochs=7, accelerator='tpu', tpu_cores=8, \n",
        "                         callbacks=[checkpointCallback], profiler='simple', \n",
        "                         auto_lr_find=True)\n",
        "\n",
        "trainer.fit(model, datamodule=ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19016552",
      "metadata": {
        "papermill": {
          "duration": 0.005237,
          "end_time": "2022-06-15T01:02:29.771995",
          "exception": false,
          "start_time": "2022-06-15T01:02:29.766758",
          "status": "completed"
        },
        "tags": [],
        "id": "19016552"
      },
      "source": [
        "# Utility Functions\n",
        "\n",
        "Here we define three utility functions:\n",
        "  1. **save_images**(model, loader, folder, device)\n",
        "      - This is used to save predictions and their corresponding masks from a specified model across a specified dataloader.\n",
        "      - Device can be set to cpu if not running on gpu\n",
        "  2. **get_concat_v**(im1, im2)\n",
        "      - This is used to later concatenate the prediction and mask images we create using save_image.\n",
        "      - im1 & im2 are PIL.Image objects\n",
        "  3. **merge_photos**(src_folder, dst_folder, remove_single)\n",
        "      - This is used to read and concatenate the prediction and mask images using the previously defined get_concat_v().\n",
        "      - remove_single can be set to False to save the unmerged images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c707152a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-15T01:02:29.784363Z",
          "iopub.status.busy": "2022-06-15T01:02:29.783638Z",
          "iopub.status.idle": "2022-06-15T01:02:29.794942Z",
          "shell.execute_reply": "2022-06-15T01:02:29.794247Z"
        },
        "papermill": {
          "duration": 0.019488,
          "end_time": "2022-06-15T01:02:29.796838",
          "exception": false,
          "start_time": "2022-06-15T01:02:29.777350",
          "status": "completed"
        },
        "tags": [],
        "id": "c707152a"
      },
      "outputs": [],
      "source": [
        "def save_images(model, loader, folder='val_img', device='cuda'):\n",
        "    model.eval()\n",
        "    if not os.path.isdir(folder):\n",
        "        os.mkdir(folder)\n",
        "    model.to(device=device)\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        y = y.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x).cuda())\n",
        "            preds = (preds > 0.5).float()\n",
        "        torchvision.utils.save_image(\n",
        "            preds, f\"{folder}/pred_{idx}.png\"\n",
        "        )\n",
        "        torchvision.utils.save_image(y, f\"{folder}/mask_{idx}.png\")       \n",
        "\n",
        "def get_concat_v(im1, im2):\n",
        "    dst = Image.new('RGB', (im1.width, im1.height + im2.height))\n",
        "    dst.paste(im1, (0, 0))\n",
        "    dst.paste(im2, (0, im1.height))\n",
        "    return dst\n",
        "\n",
        "def merge_photos(src_folder: str='./val_img', dst_folder: str='./merged_val_img', remove_single: bool=True):\n",
        "    files = glob.glob(src_folder+'/*.png')\n",
        "    if not os.path.isdir(dst_folder):\n",
        "        os.mkdir(dst_folder)\n",
        "    for i in range(int(len(files)/2)):\n",
        "        pred_img = Image.open(f'{src_folder}/pred_{i}.png')\n",
        "        mask_img = Image.open(f'{src_folder}/mask_{i}.png')\n",
        "        get_concat_v(pred_img, mask_img).save(f'{dst_folder}/merged_pred_mask_{i}.png')\n",
        "        if remove_single:\n",
        "            os.remove(f'./val_img/pred_{i}.png')\n",
        "            os.remove(f'./val_img/mask_{i}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2b9071",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-15T01:02:29.808729Z",
          "iopub.status.busy": "2022-06-15T01:02:29.808376Z",
          "iopub.status.idle": "2022-06-15T01:03:33.023097Z",
          "shell.execute_reply": "2022-06-15T01:03:33.022038Z"
        },
        "papermill": {
          "duration": 63.223354,
          "end_time": "2022-06-15T01:03:33.025509",
          "exception": false,
          "start_time": "2022-06-15T01:02:29.802155",
          "status": "completed"
        },
        "tags": [],
        "id": "9e2b9071"
      },
      "outputs": [],
      "source": [
        "ds.setup()\n",
        "save_images(model, ds.val_dataloader())\n",
        "merge_photos()\n",
        "# TODO: Implement feature for sampling random img/label pairs for model prediction/eval\n",
        "#         - Should show output and mask as images, pref side by side or across batches\n",
        "#         - Conduct additional testing using the competition or data from the internet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ab58ee",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-06-15T01:03:33.039072Z",
          "iopub.status.busy": "2022-06-15T01:03:33.038726Z",
          "iopub.status.idle": "2022-06-15T01:03:53.748478Z",
          "shell.execute_reply": "2022-06-15T01:03:53.747470Z"
        },
        "papermill": {
          "duration": 20.719042,
          "end_time": "2022-06-15T01:03:53.750541",
          "exception": false,
          "start_time": "2022-06-15T01:03:33.031499",
          "status": "completed"
        },
        "tags": [],
        "id": "26ab58ee",
        "outputId": "f78acbf6-3add-44a0-fbe8-0e6d2bbbcdcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: lightning_logs/ (stored 0%)\r\n",
            "  adding: lightning_logs/version_0/ (stored 0%)\r\n",
            "  adding: lightning_logs/version_0/hparams.yaml (stored 0%)\r\n",
            "  adding: lightning_logs/version_0/events.out.tfevents.1655251958.ba12c49bf83b.23.0 (deflated 66%)\r\n",
            "  adding: merged_val_img/ (stored 0%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_12.png (deflated 26%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_18.png (deflated 25%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_35.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_34.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_20.png (deflated 29%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_5.png (deflated 29%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_31.png (deflated 30%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_24.png (deflated 27%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_38.png (deflated 26%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_56.png (deflated 15%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_16.png (deflated 25%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_1.png (deflated 33%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_36.png (deflated 27%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_42.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_49.png (deflated 32%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_22.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_47.png (deflated 24%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_3.png (deflated 24%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_39.png (deflated 22%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_15.png (deflated 29%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_8.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_54.png (deflated 21%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_52.png (deflated 26%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_2.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_30.png (deflated 32%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_50.png (deflated 31%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_11.png (deflated 25%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_0.png (deflated 26%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_10.png (deflated 22%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_27.png (deflated 30%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_53.png (deflated 25%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_51.png (deflated 25%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_44.png (deflated 22%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_21.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_29.png (deflated 36%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_37.png (deflated 27%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_43.png (deflated 22%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_19.png (deflated 26%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_6.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_41.png (deflated 26%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_48.png (deflated 29%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_9.png (deflated 27%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_33.png (deflated 24%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_7.png (deflated 31%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_25.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_13.png (deflated 27%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_23.png (deflated 25%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_26.png (deflated 26%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_40.png (deflated 31%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_4.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_45.png (deflated 29%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_17.png (deflated 22%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_32.png (deflated 30%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_55.png (deflated 28%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_14.png (deflated 23%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_28.png (deflated 25%)\r\n",
            "  adding: merged_val_img/merged_pred_mask_46.png (deflated 26%)\r\n",
            "  adding: unet_3/ (stored 0%)\r\n",
            "  adding: unet_3/epoch=7-val_loss=0.01692.ckpt (deflated 8%)\r\n"
          ]
        }
      ],
      "source": [
        "!zip -r lightning_logs.zip ./lightning_logs\n",
        "!zip -r merged_val_img.zip ./merged_val_img\n",
        "!zip -r unet.zip ./unet_3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c54c5cb0",
      "metadata": {
        "papermill": {
          "duration": 0.005773,
          "end_time": "2022-06-15T01:03:53.762813",
          "exception": false,
          "start_time": "2022-06-15T01:03:53.757040",
          "status": "completed"
        },
        "tags": [],
        "id": "c54c5cb0"
      },
      "source": [
        "Since Kaggle doesn't work with Tensorboard, download and load the logs in Google Collab to analyze the training and validation metrics\n",
        "\n",
        "<button><a href='./lightning_logs.zip'>Download Logs</a></button>\n",
        "<button><a href='./merged_val_img.zip'>Download Images</a></button>\n",
        "<button><a href='./unet.zip'>Download Model</a></button>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3151.777829,
      "end_time": "2022-06-15T01:03:57.222328",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-06-15T00:11:25.444499",
      "version": "2.3.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}